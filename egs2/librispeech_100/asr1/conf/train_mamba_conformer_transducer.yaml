############################################################
# LibriSpeech-100h | Transducer
# encoder = mamba , predictor (decoder) = conformer
############################################################

#################################
# Front-end & data augmentation
frontend: default
frontend_conf:
  n_mels: 80
  fs: 16000
  n_fft: 512
  win_length: 400
  hop_length: 160

specaug: specaug
specaug_conf:
  apply_time_warp: true
  time_warp_window: 5
  time_warp_mode: bicubic
  apply_freq_mask: true
  freq_mask_width_range: [0, 27]
  num_freq_mask: 2
  apply_time_mask: true
  time_mask_width_ratio_range: [0.0, 0.05]
  num_time_mask: 5

#################################
encoder: mamba
encoder_conf:
  output_size: 256
  num_blocks: 12
  dropout_rate: 0.1
  pos_enc_dropout_rate: 0.1
  input_layer: conv2d
  ssm_cfg:
    d_state: 16
    expansion_factor: 2
    conv_kernel: 5
  init_rescale: true       # 論文推奨の rescale 初期化

#################################
decoder: conformer
decoder_conf:
  embed_dim: 256
  num_blocks: 2
  linear_units: 1024
  dropout_rate: 0.1
  selfattention_layer_type: rel_selfattn
  positionwise_layer_type: linear
  normalize_before: true

#################################
# Joint network
joint_net_conf:
  joint_space_size: 512
  joint_activation_type: tanh

#################################
# Model-level
model_conf:
  ctc_weight: 0.0          # 純粋 Transducer
  transducer_weight: 1.0
  auxiliary_ctc: false
  fastemit_lambda: 0.0     # 必要なら fastemit 正則化

#################################
# Optimisation
optim: adam
optim_conf:
  lr: 0.002
  weight_decay: 0.000001
scheduler: warmuplr
scheduler_conf:
  warmup_steps: 15000

seed: 2022
num_workers: 8
init: none
best_model_criterion:
  - - valid
    - loss
    - min
keep_nbest_models: 10
use_amp: false
batch_type: numel
batch_bins: 1000000        # 24e5 ≒ 3 GB/GPU 目安
accum_grad: 64
max_epoch: 70
patience: none
############################################################